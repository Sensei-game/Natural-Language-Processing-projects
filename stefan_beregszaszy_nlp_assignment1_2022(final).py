# -*- coding: utf-8 -*-
"""Stefan_Beregszaszy_NLP_assignment1-2022(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-mzYbhAFbITLkYwCnVSbPryWH3BpmD5

# [COM6513] Assignment 1: Sentiment Analysis with Logistic Regression

### Instructor: Nikos Aletras


The goal of this assignment is to develop and test a **text classification** system for **sentiment analysis**, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).



For that purpose, you will implement:


- Text processing methods for extracting Bag-Of-Word features, using 
    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). 
    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** 
    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**1 mark**) **Tip: you should merge the two representations**



- Binary Logistic Regression (LR) classifiers that will be able to accurately classify movie reviews trained with: 
    - (1) BOW-count (raw frequencies) 
    - (2) BOW-tfidf (tf.idf weighted)
    - (3) BOCN-count
    - (4) BOCN-tfidf
    - (5) BOW+BOCN (best performing weighting; raw or tfidf)



- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:
    - Minimise the Binary Cross-entropy loss function (**1 mark**)
    - Use L2 regularisation (**1 mark**)
    - Perform multiple passes (epochs) over the training data (**1 mark**)
    - Randomise the order of training data after each pass (**1 mark**)
    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)
    - After each epoch print the training and development loss (**1 mark**)



- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**2 marks). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**


- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**1 mark**). 


- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!). If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks**)


- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**2 marks**). 


- Provide efficient solutions by using Numpy arrays when possible (you can find tips in Lab 1 sheet). Executing the whole notebook with your code should not take more than 5 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs (**2 marks**). 






### Data 

The data you will use are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:

- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.
- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.
- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.




### Submission Instructions

You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).

You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. 

There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\% or higher. The quality of the analysis of the results is as important as the accuracy itself. 

This assignment will be marked out of 20. It is worth 20\% of your final grade in the module.

The deadline for this assignment is **23:59 on Mon, 14 Mar 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.
"""

import pandas as pd
import numpy as np
from collections import Counter
import re
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import random
import string

# fixing random seed for reproducibility
random.seed(123)
np.random.seed(123)

"""## Load Raw texts and labels into arrays

First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes).
"""

# read csv file with the test sets
test = pd.read_csv("data_sentiment/test.csv")



# displaying the list of column names
#Column 1 = TEXT
#Column 2 = LABELS

# creating a list of column names by
# calling the columns
test_column_names = list(test.columns)

"""If you use Pandas you can see a sample of the data."""

# read csv file with the train sets
train = pd.read_csv("data_sentiment/train.csv")



# displaying the list of column names
#Column 0 = TEXT
#Column 1 = LABELS  


# creating a list of column names by
# calling the .columns
train_column_names = list(train.columns)


###########################################################


# read csv file with the development sets
dev  = pd.read_csv("data_sentiment/dev.csv")


# displaying the list of column names
#Column 0 = TEXT
#Column 1 = LABELS


# creating a list of column names by
# calling the .columns
dev_column_names = list(dev.columns)

"""The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:

"""

#put the trainning raw texts into Python lists
train_text = list(train[train_column_names[0]])

#print the text for verification
#print(train_text,"\n")

#put the trainning labels into a NumPy arrays
train_label = train[train_column_names[1]].values

#print the train label for verification
print(train_label,"\n")


##############################################


#put the testing raw texts into Python lists
test_text = list(test[test_column_names[0]])

#print the text for verification
#print(test_text,"\n")

#put the testing labels into a NumPy arrays
test_label = test[test_column_names[1]].values

#print the test label for verification
print(test_label,"\n")


###############################################


#put the development raw texts into Python lists
dev_text = list(dev[dev_column_names[0]])

#print the text for verification
#print(dev_text,"\n")

#put the development labels into a NumPy arrays
dev_label = dev[dev_column_names[1]].values

#print the dev label for verification
print(dev_label,"\n")

"""# Vector Representations of Text 


To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).


## Text Pre-Processing Pipeline

To obtain a vocabulary of features, you should: 
- tokenise all texts into a list of unigrams (tip: using a regular expression) 
- remove stop words (using the one provided or one of your preference) 
- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)
- remove ngrams appearing in less than K documents
- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.

"""

#make a list for unigrams from each
def generate_ngrams(s, n):
   
    # Convert to lowercases
    #s = s.lower()
    
    # Replace all none alphanumeric characters with spaces
    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(s))  
    
    # Break sentence in the token, remove empty tokens
    tokens = [token for token in s.split(" ") if token != ""]
    
    # Use the zip function to help generate n-grams
    # Concatentate the tokens into ngrams and return
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]

#Generate unigrams, using the trainning set
train_unigram = generate_ngrams(train_text, n=1)

#Generate unigrams, using the testing set
test_unigram = generate_ngrams(test_text, n=1)

#Generate unigrams, using the development set
dev_unigrams = generate_ngrams(dev_text, n=1)




stop_words = ['a','in','on','at','and','or', 
              'to', 'the', 'of', 'an', 'by', 
              'as', 'is', 'was', 'were', 'been', 'be', 
              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',
             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',
              'do', 'did', 'can', 'could', 'who', 'which', 'what', 
             'his', 'her', 'they', 'them', 'from', 'with', 'its']


#Remove these stop words from the list of ngrams
def remove_stop_word(ngram):
    
    #use list comprehension,
    #to only return words not inlcuded in stop_words
    return [word for word in ngram if word not in stop_words]

#Remove stopwords on the train/test/dev unigrams
uni_train_no_sw = remove_stop_word(train_unigram)
uni_test_no_sw = remove_stop_word(test_unigram)
uni_dev_no_sw = remove_stop_word(dev_unigrams)
    
#Make Bigrams from the created unigrams
bi_train = generate_ngrams(uni_train_no_sw, n=2)
bi_test = generate_ngrams(uni_test_no_sw, n=2)
bi_dev = generate_ngrams(uni_dev_no_sw, n=2)

#Make Trigrams from the created unigrams
tri_train = generate_ngrams(uni_train_no_sw, n=3)
tri_test = generate_ngrams(uni_test_no_sw, n=3)
tri_dev = generate_ngrams(uni_dev_no_sw, n=3)


#####################################
#Remove ngrams appearing in less than K documents
def doc_counter(set_train, set_test, dev_test):
    
    #use Counter type in order to count all unique words, 
    #from each train/test/dev set 
    c = Counter()
    c.update(set_train)
    c.update(set_test)
    c.update(dev_test)
    
    return c

#Initialise the set versions of the train/test/dev unigrams
set_uni_train = set(uni_train_no_sw)
set_uni_test = set(uni_test_no_sw)
set_uni_dev = set(uni_dev_no_sw)

#Call doc_counter for all the unigram sets
uni_doc_appearances = doc_counter(set_uni_train, set_uni_test, set_uni_dev)

#Initialise the set versions of the train/test/dev bigrams
set_bi_train = set(bi_train)
set_bi_test = set(bi_test)
set_bi_dev = set(bi_dev)

#Call doc_counter for all the bigram sets
bi_doc_appearances = doc_counter(set_bi_train, set_bi_test, set_bi_dev)


set_tri_train = set(tri_train)
set_tri_test = set(tri_test)
set_tri_dev = set(tri_dev)

#Initialise the set versions of the train/test/dev trigrams
tri_doc_appearances = doc_counter(set_tri_train, set_tri_test, set_tri_dev)


#Number of documents(set between 1 and 3)
def find_words(c, k):

    #Output list variable
    found_words =[]
    
    #c is a Counter, 
    # go through every word contained by c
    for words in c.keys(): 
    
    #if documents appearance value is smaller than k
    #in that case continue
        if c[words] < k :
            continue
        else:
            #Add this ngram to the list
            found_words.append(words)
             
    #No need to keep it as a list, 
    # arrays will help with efficiency
    return np.array(found_words)


#Function that will return lists,
#containning only ngrams  appearing at least in K documents
def remove_k(set_train,  doc_ap):
    #set_test, set_dev
    clean_train = []
    clean_test = []
    clean_dev = []
    
    print("Finding words....", "\n") 
    
    #Get all ngrams that appear in at least k documents
    found_words = find_words(doc_ap, k=3)
    
    print(found_words)
    
    print("Starting trainning....", "\n")
    clean_train = [word for word in set_train if word in found_words]
                
    print("Starting testing....", "\n")    
    # clean_test = [word for word in set_test if word in found_words]
        
    # print("Starting dev....", "\n") 
    # clead_dev = [word for word in set_dev if word in found_words]
    
    return np.array(clean_train)#, np.array(clean_test), np.array(clean_dev)

"""# *This code cell is separated from the rest due to the heavy computation needed.*

 # **Only run this cell once!**
 
 # *Estimated processing time with the full lists of ngrams: 12 minutes.*
 
 
 # *5 Minutes with the sets version*
"""

#Remove all ngrams that exist in less than k documents
'''Will only go through the trainning set ngrams'''

'''In order to see the results on other datasets,....'''

'''Un-comment the other sets and add the desired clean_nGRAM_DATASET'''


print("Starting UNIGRAMS....", "\n")


clean_uni_train  = remove_k(set_uni_train,
                                                              # set_uni_test,
                                                              # set_uni_dev,
                                                            uni_doc_appearances)
# clean_uni_test, clean_uni_dev 

print("Starting BIGRAMS....", "\n")


clean_bi_train  = remove_k(set_bi_train,
                                                        # set_bi_test,
                                                        # set_bi_dev,
                                                          bi_doc_appearances)
# clean_bi_test, clean_bi_dev

print("Starting TRIGRAMS....", "\n")


clean_tri_train  = remove_k(set_tri_train,
                                                        #  set_tri_test,
                                                        #  set_tri_dev,
                                                           tri_doc_appearances)
# clean_tri_test, clean_tri_dev

#Create a vocabulary of unigrams, bigrams and trigrams

#Will only go through those of the trainning set
vocab = set(clean_uni_train)

# vocab.update(clean_uni_test)
# vocab.update(clean_uni_dev)

vocab.update(clean_bi_train)

# vocab.update(clean_bi_test)
# vocab.update(clean_bi_dev)

vocab.update(clean_tri_train)

# vocab.update(clean_tri_test)
# vocab.update(clean_tri_dev)



print(vocab, "\n")

"""### N-gram extraction from a document

You first need to implement the `extract_ngrams` function. It takes as input:
- `x_raw`: a string corresponding to the raw text of a document
- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.
- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.
- `stop_words`: a list of stop words
- `vocab`: a given vocabulary. It should be used to extract specific features.
- `char_ngrams`: boolean. If true the function extracts character n-grams

and returns:

- `x': a list of all extracted features.

See the examples below to see how this function should work.
"""

def extract_ngrams(x_raw, ngram_range, token_pattern, 
                   stop_words, vocab, char_ngrams):
    
    #Set the smallest value of ngram types
    min_ = ngram_range[0]
    
    #Set the biggest value of ngram types
    max_ = ngram_range[-1]
    
    #Initialise output values
    output_ngram = []
    output_char_gram =[]

    #Produce Character ngrams or regular ngrams
    if char_ngrams == False:
        
        #Go through every type of ngram(i.e. unigram, bigram)
        for rn in range(min_,max_+1):
            print(rn)    
    
            # Replace all none alphanumeric characters with spaces
            x_sub = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(x_raw))

            x_sub.replace("'", " ")
    
            # Break sentence in the token, remove empty tokens
            tokens = [token for token in x_sub.split(token_pattern) if token != ""]
    
            # Use the zip function to help generate n-grams
            # Concatentate the tokens into ngrams and return
            ngrams = zip(*[tokens[i:] for i in range(rn)])
            final_ngrams = [" ".join(ngram) for ngram in ngrams]

            
            #Remove stop words from ngrams
            no_stop_ngram = [word for word in final_ngrams if word not in stop_words]
            #if rn == 3:
                #print('This is the stop_words',*no_stop_ngram, sep = "', '" )
            
            #filter ngrams in vocabulary
            for word_o in no_stop_ngram:
                if word_o in vocab:
                    output_ngram.append(word_o)

        print(output_ngram)    
        return output_ngram
            
    else:

    #Generate character ngrams
    
    #Go through every type of ngram(i.e. unigram, bigram)
        for rn in range(min_,max_+1):

            final_char =[]
            #b[i:i+n] for i in range(len(b)-n+1)
            
            # Replace all none alphanumeric characters with spaces
            x_sub = re.sub(r"[^a-zA-Z0-9\s]", "", str(x_raw))
            
            x_sub.replace("'", "")
            x_sub.replace(" ","")            
            # tokens = [token for token in x_sub.split(" ") if token != ""]

            
            # Use the zip function to help generate character n-grams 
            # Concatentate the tokens into ngrams and return
            char_grams = zip(*[x_sub[i:] for i in range(rn)])
            
             #Split words by character, not by whitespace
            final_char = ["".join(char_gram) for char_gram in char_grams]
            
            
            #Remove stopwords
            output_char_gram = [word for word in final_char if word not in stop_words]
        
        print(output_char_gram)         
        return output_char_gram

"""Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`

For extracting character n-grams the function should work as follows:
"""

""" Will Keep this cell commented, for preview purposes """

""" To check the real running code go to the cell above """

# def extract_ngrams(x_raw="movie", 
#                ngram_range=(2,4), 
#                stop_words=[],
#                char_ngrams=True):
    
#     min = ngram_range[0]
    
#     max = ngram_range[-1]
    
#     output_char_gram, no_stop_char =[]

    
#     for rn in range(min,max+1):

#             #b[i:i+n] for i in range(len(b)-n+1)
            
#             # Replace all none alphanumeric characters with spaces
#             x_sub = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(x_raw))
            
#             # Break sentence in the token, remove empty tokens
#            # tokens = [token for token in x_sub.split(token_pattern) if token != ""]
            
#             # Use the zip function to help generate character n-grams 
#             # Concatentate the tokens into ngrams and return
#             char_grams = zip(*[x_sub[i:] for i in range(rn)])
#             final_char = ["".join(char_gram) for char_gram in char_grams]
            
            
#             #Remove stopwords
#             no_stop_char = [word for word in final_char if word not in stop_words]
        

#             #search in vocab
#             output_char_gram = [word_o for word_o in no_stop_char if word_o in vocab]
            
    
    
#     return output_char_gram

"""### Create a vocabulary 

The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:
- `X_raw`: a list of strings each corresponding to the raw text of a document
- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.
- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.
- `stop_words`: a list of stop words
- `min_df`: keep ngrams with a minimum document frequency.
- `keep_topN`: keep top-N more frequent ngrams.

and returns:

- `vocab`: a set of the n-grams that will be used as features.
- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.
- `ngram_counts`: counts of each ngram in vocab

Hint: it should make use of the `extract_ngrams` function.
"""

#@title
def get_vocab(X_raw, ngram_range, token_pattern, 
              min_df, keep_topN, stop_words, char_ngrams):
    
    #Set the smallest value of ngram types
    min_ = ngram_range[0]
    
    #Set the biggest value of ngram types
    max_ = ngram_range[-1]
    
    ngrams = np.array([0])

    if char_ngrams ==False:

        #Go through every type of ngram(i.e. unigram, bigram)
        for rn in range(min_, max_+1):
            #     n = ngram-range
            print("Filter rn.... ", rn ,"\n")
            
        
            special_char=[",",":"," ",";",".","?","'"]

            # Replace all none alphanumeric characters with spaces
            s = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(X_raw))

            # Break sentence in the token, remove empty tokens
            tokens = [token for token in s.split(" ") if token != ""]

            # Use the zip function to help generate n-grams
            # Concatentate the tokens into ngrams and return
            n_grams = zip(*[tokens[i:] for i in range(rn)])
            ngrams = np.append(ngrams, [" ".join(ngram) for ngram in n_grams])


        print("Filter vocab....","\n")
        
        #Remove stop words and special charcters from the list of ngrams
        filtered_vocab = [w for w in ngrams if w not in stop_words and w not in special_char]
        
        
        
        print("Start extract_ngram....","\n")
        
        #Initialise and pass the filtered vocab as a set
        original_vocab = set(filtered_vocab)
        
        #Extract ngrams from the vocabulary
        ngram = extract_ngrams(X_raw, ngram_range, token_pattern, stop_words, original_vocab,char_ngrams)
    
    else:
        
        #Go through every type of ngram(i.e. unigram, bigram)
        for rn in range(min_, max_+1):
            #     n = ngram-range
            print("Filter rn.... ", rn ,"\n")
            
        
            special_char=[",",":"," ",";",".","?","'"]

            # Replace all none alphanumeric characters with spaces
            s = re.sub(r'[^a-zA-Z0-9\s]', '', str(X_raw))

            s.replace("'", "")
            s.replace(" ","")  

            # Use the zip function to help generate n-grams
            # Concatentate the tokens into ngrams and return
            n_grams = zip(*[s[i:] for i in range(rn)])
            ngrams = np.append(ngrams, ["".join(ngram) for ngram in n_grams])


        print("Filter vocab....","\n")
        
        #Remove stop words and special charcters from the list of ngrams
        filtered_vocab = [w for w in ngrams if w not in stop_words and w not in special_char]
        
        
        
        print("Start extract_c_gram....","\n")
        
        #Initialise and pass the filtered vocab as a set
        original_vocab = set(filtered_vocab)
        
        #Extract ngrams from the vocabulary
        ngram = extract_ngrams(X_raw, ngram_range, token_pattern, stop_words, original_vocab,char_ngrams)

 ####################### FUNCTIONAL #####################  
 
    df = Counter()
    df.update(ngram)
    
    df_final = Counter()
    
    #Keep the ngrams with a minimum frequency
    for word in df.elements():
        if df[word] >= min_df:
            df_final[word] = df[word]
    
    

    
    vocab = set()
    ngram_counts = []
    df ={}
    
    for word, count in df_final.most_common(keep_topN):
        vocab.add(word)
        ngram_counts.append(count)#Count is regular count
#         df_dict.update({word: count}) 
    
    def Compute_DF(ngrams):
        
        DF = {}
        
        print("Started DFs small.. \n")
        for i in range(len(ngrams)):
           
            for w in ngrams[i]:
                try:
                    DF[w].add(i)
                except:
                    DF[w] = {i}


        for i in DF:
            DF[i] = len(DF[i])

        return DF
    
    
    def find_doc_freq(word,DF):
        
        #Method to get a specific ngram's Document frequency
        
        c = 0
        
        #get the documnet frequency of a specific ngram
        try:
            c = DF[word]
        except:
            pass
        return c
    
    
    #Calculate document frequency of all ngrams
    DF = Compute_DF(ngram)

    found_gram = np.array([0])

    #Filter ngrams through vocabulary 
    found_gram = [w for w in ngram if w in vocab]

    #Only go through the ngrams found in vocab
    N = len(found_gram)
        
    for i in range(N):
            
        tokens = found_gram[i]
        counter = Counter(tokens)#Replace with count vector
        words_count = len(tokens)

        for token in np.unique(tokens):
            tf = counter[token]/words_count
            df_word = find_doc_freq(token,DF)
            df.update({token: df_word}) 
    
    
    
    # print(vocab)
    # print(df)
    # print(ngram_counts)

    
    return vocab, df, ngram_counts

"""Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"""

test_vocab, test_df, test_count = get_vocab(test_text, ngram_range=(1,3), token_pattern=r' ', 
                       min_df=2, keep_topN=500, 
                       stop_words = stop_words,char_ngrams = False)

"""Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:"""

def create_2dict(df_dict):
    id2word = {}
    word2id = {}
    dic_id = 0

    for word in test_df.keys():

            #(1) vocabulary id -> word
            id2word.update({dic_id : word}) 

            # (2) word -> vocabulary id
            word2id.update({word: dic_id}) 

            dic_id += 1 

    print('Dictionary [ID : WORD] : ',id2word, "\n")
    print('Dictionary [WORD : ID] : ',word2id, "\n")
    
    return id2word , word2id

"""Now you should be able to extract n-grams for each text in the training, development and test sets:"""

#TEST
# test_vocab, test_df, test_count = get_vocab(test_text, ngram_range=(1,3), token_pattern=r' ', 
#                        min_df=2, keep_topN=500, 
#                        stop_words = stop_words)

#train
train_vocab, train_df, train_count = get_vocab(train_text, ngram_range=(1,3), token_pattern=r' ', 
                       min_df=10, keep_topN=100, 
                       stop_words=stop_words,char_ngrams=False)
#Dev
dev_vocab, dev_df, dev_count = get_vocab(dev_text, ngram_range=(1,3), token_pattern=r' ', 
                       min_df=10, keep_topN=100, 
                       stop_words=stop_words,char_ngrams=False)

print("Test Dictionary >>> ", "\n")
id_w_test, w_id_test = create_2dict(test_df)

print("Train Dictionary >>> ", "\n")
id_w_train, w_id_train =create_2dict(train_df)

print("DEV Dictionary >>> ", "\n")
id_w_dev, w_id_dev =create_2dict(dev_df)

"""## Vectorise documents

Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:
- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`
- `vocab`: a set of n-grams to be used for representing the documents

and return:
- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.
"""

def vectorise(X_ngram, vocab):

  vector = []
  #For every ngram 
  for i in X_ngram:
  
    #get the raw frequency of a ngram
    c = Counter(i)
    empty = []
    
    #Only take those from the vocabulary
    empty = [c[w] for w in vocab]
        
    vector.append(empty)
            
  # print(vector)    
  return np.array(vector)

"""Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:

#### Count vectors
"""

# Use Extract ngram train_ngram
train_ngram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=False)for i in train_text) 
train_ngram

dev_ngram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=False)for i in dev_text) 
dev_ngram

# extract_ngrams(x_raw, ngram_range, token_pattern, 
#                    stop_words, vocab, char_ngrams)

# print("Vectorise test text....","\n")
# test_count, test_vect = vectorise(test_text, test_vocab)



#List of Ngrams, SET_UNIGRAMS
print("Vectorise train text....","\n")
train_count = vectorise(train_ngram, train_vocab)
# train_count



# , train_vect

print("Vectorise dev text....","\n")
dev_count = vectorise(dev_ngram, dev_vocab)
# dev_count

#, dev_vect

"""#### TF.IDF vectors

First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)
"""

def Compute_IDF(n_gram, df):

    #df contains document frequencies of ngrams inside of vocab
    print(df.values())

    #Ngram is a count vector
    
    N = len(n_gram)
    
    idf = [np.log10(N/(i+1)) for i in df.values()]#numerator is added 1 to avoid negative or 0 values

    return idf

#use raw text
train_vect_idf = Compute_IDF(train_text,train_df)
train_vect_idf

"""Then transform your count vectors to tf.idf vectors:"""

# tf = counter[token]/words_count
 # Takes a count vector
def compute_tf_IDF(count_vect,idf):
        
        tf_idf = np.log10(1 + count_vect)*idf

        return tf_idf

# tf_idf[doc, token] = tf*idf
# TF_IDF uses count vector
train_vect_tfidf = compute_tf_IDF(train_count, train_vect_idf)
train_vect_tfidf

"""# Binary Logistic Regression

After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment.

First, you need to implement the `sigmoid` function. It takes as input:

- `z`: a real number or an array of real numbers 

and returns:

- `sig`: the sigmoid of `z`
"""

def sigmoid(z):
    
    sig = 1 / (1 + np.exp(-z))
    return sig

"""Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:

- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \times |vocab|$)
- `weights`: a 1-D array of the model's weights $(1, |vocab|)$

and returns:

- `preds_proba`: the prediction probabilities of X given the weights
"""

def predict_proba(X, weights):
    
    preds_proba = sigmoid(np.dot(X, weights))
              
        
    return preds_proba

"""Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:

- `X`: an array of documents represented by bag-of-ngram vectors ($N \times |vocab|$)
- `weights`: a 1-D array of the model's weights $(1, |vocab|)$

and returns:

- `preds_class`: the predicted class for each x in X given the weights
"""

def predict_class(X, weights):
    
    """
        Predict the class between 0 and 1 using learned logistic regression parameters theta.
        Using threshold value 0.5 to convert probability value to class value 

        Input:
        ----------
        X : 2D array where each row represents a docuemnt  and each column represent the feature ndarray. Dimension(N x |vocab|)
            
        weights : 1D array of weights. Dimension (1 x |vocab|)

        Output:
        -------
        Class type based on threshold
        """

    p = predict_proba(X,weights) >= 0.5
    
    preds_class = p.astype(int)
    
#   if y_pred_tr>=0.5: #LABELS
            
#       predictions.append(1)
#   else:
#       predictions.append(0)

    return preds_class

"""To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:

- `X`: input vectors
- `Y`: labels
- `weights`: model weights
- `alpha`: regularisation strength

and return:

- `l`: the loss score
"""

def binary_loss(X, Y, weights, alpha):
 
    """
#         Compute cost for logistic regression.

#         Input:
#         ----------
#         X : 2D array where each row represents a document  and each column represents the vocab size. Dimension(N x |vocab|)
#            
#         y : 1D array of labels/target value for each traing example. dimension(1 x |vocab|)

#         weights : 1D array of fitting parameters or weights. Dimension (1 x n)

#         alpha: regularisation strengths to be added when calculating the loss function

#         Output:
#         -------
#         J : The cost of using theta as the parameter for linear regression to fit the data points in X and y.
#         """


    m = len(X)                
    yhat = sigmoid(np.dot(X, weights) + alpha)   
    
    predict = Y * np.log(yhat) + (1 - Y) * np.log(1 - yhat) 
    
    # l = -sum(predict) / m
    
    l = -(predict).mean()


    return l

"""Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:

- `X_tr`: array of training data (vectors)
- `Y_tr`: labels of `X_tr`
- `X_dev`: array of development (i.e. validation) data (vectors)
- `Y_dev`: labels of `X_dev`
- `lr`: learning rate
- `alpha`: regularisation strength
- `epochs`: number of full passes over the training data
- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold
- `print_progress`: flag for printing the training progress (train/validation loss)


and returns:

- `weights`: the weights learned
- `training_loss_history`: an array with the average losses of the whole training set after each epoch
- `validation_loss_history`: an array with the average losses of the whole development set after each epoch
"""

def SGD(X_tr, Y_tr, X_dev, Y_dev, lr, 
        alpha, epochs, 
        tolerance, print_progress):
            
        
    training_loss_history = np.array([0])
    validation_loss_history = np.array([0])
        
    training_loss_prev = np.array([0])
    validation_loss_prev = np.array([0])
    
    training_loss_current = np.array([0])
    validation_loss_current = np.array([0])
        
    # weigths initialised
    m = np.zeros(X_tr.shape[1])
    
    train_set = list(zip(X_tr,Y_tr))

    # for every epoch
    for epoch in range(epochs):
        
        ####TRAINNING####
        # for every data point(X_tr,Y_tr)
        np.random.shuffle(train_set)

        for x_i, y_i in train_set:

          m -= lr*(x_i*(predict_proba(x_i,m) - y_i) + 2*alpha*m)

        loss_training = binary_loss(X_tr,Y_tr,m,alpha)
        loss_dev = binary_loss(X_dev,Y_dev,m,alpha)

            
        

        if training_loss_prev == np.array([0]):
            
            training_loss_prev = binary_loss(X_tr,Y_tr,m,alpha)
            training_loss_history = np.append(training_loss_history, training_loss_prev)
            
        else:
           
            training_loss_current = binary_loss(X_tr,Y_tr,m,alpha)
            
            if (training_loss_current - training_loss_prev) >= tolerance: 
            
                training_loss_history = np.append(training_loss_history, training_loss_current)
                training_loss_prev = training_loss_current
        
        
        
        if print_progress == True:
            print("Loss after "+ str(epoch) + " steps is: "+ str(training_loss_history))
                      

        
        
        if validation_loss_prev == np.array([0]):
            
            validation_loss_prev = binary_loss(X_dev,Y_dev,m,alpha)
            validation_loss_history = np.append(validation_loss_history, validation_loss_prev)
            
        else:
           
            validation_loss_current = binary_loss(X_dev,Y_dev,m,alpha)
            
            if (validation_loss_current - validation_loss_prev) >= tolerance: 
            
                validation_loss_history = np.append(validation_loss_history, validation_loss_current)
                validation_loss_prev = validation_loss_current
        
        
        
        if print_progress == True:
            print("Loss after "+ str(epoch) + " steps is: "+ str(validation_loss_history))
        

    
    if print_progress == True:
        print("Final Loss after "+ str(epoch) + " steps is: "+ str(training_loss_history),"\n")
        print("Final Loss after "+ str(epoch) + " steps is: "+ str(validation_loss_history),"\n")
        print("Final weights : ", m,"\n")  
    
    return m, training_loss_history, validation_loss_history

# print(type(train_vect))
# print(np.shape(train_label))
# print(train_count)

print(dev_count)

print(np.shape(train_count))

print(np.shape(dev_count))

"""## Train and Evaluate Logistic Regression with Count vectors

First train the model using SGD:
"""

# lr=[0.01, 0.001]
# alpha=[]

#BOW-count
#Hyper-parameter Tuning



weights, training_loss_history, validation_loss_history = SGD(train_count, train_label,dev_count, dev_label, lr=0.001,
                                                              alpha=0.001, epochs=100, 
                                                              tolerance=0.0001, print_progress=True)

"""Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."""

######################## MAIN ######################
# training_loss_history
# validation_loss_history

plt.figure(figsize=(25,6))

plt.title('Cost Function Graph')
plt.plot(training_loss_history, label='Training Loss History')
plt.plot(validation_loss_history, label='Validation Loss History')
plt.legend(prop={'size': 16})
plt.xlabel('Number of Iterations')
plt.ylabel('Error Values')
plt.show()

"""Explain here...

## Underfit
 
#  Trainning loss History is greatly underfited, especially when compared with the Validation Loss History.

> #   The Graph also represents a sharp uprise rather than a smooth slope, this means that the Binary Logistic Regression is actually increasing after every value.



> #   Trainning Loss History is also way more shorter than it's validation counterpart, this can be only the result of multiple values being very similar to one another when produced by the bynary loss function.

#### Evaluation

Compute accuracy, precision, recall and F1-scores:
"""

# print("Vectorise test text....","\n")
test_ngram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=False)for i in test_text) 
test_ngram

test_vocab, test_df, test_count = get_vocab(test_text, ngram_range=(1,3), token_pattern=r' ', 
                                            min_df=2, keep_topN=500, 
                                            stop_words = stop_words,char_ngrams = False)

test_count = vectorise(test_ngram, test_vocab)

X_te_count = test_count

w_count = weights

preds_te_count = predict_class(X_te_count, w_count)

# train_count, weights


Y_te = test_label

print('Accuracy:', accuracy_score(Y_te,preds_te_count))
print('Precision:', precision_score(Y_te,preds_te_count))
print('Recall:', recall_score(Y_te,preds_te_count))
print('F1-Score:', f1_score(Y_te,preds_te_count))

"""Finally, print the top-10 words for the negative and positive class respectively."""

top_neg = w_count.argsort()[:10]
for i in top_neg:
#     print(id2word[i])
    print(id_w_train[i])

top_pos = w_count.argsort()[::-1][:10]
for i in top_pos:
#     print(id2word[i])
    print(id_w_train[i])

"""If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?

# Great Challenge

 Using this classifier in real case scenarion, would represent a challenge, since language will be vary greatly between users. In addition, the current classifier can only asses sentiment at a rudimentary and limited level.

 It is quite likely, for this model to asses most of the words used in niche situations very poorly.

  If said classifier were to receive custom made datasets, which would include different combinations of sentiment expression used by the current consumers, be they from a laptop retail website or on the review page of a restaurant.

>  Best features to be picked up by the classifier, would be those related to the relevant context and/or products or services(i.e. Restaurant: Food, atmosphere, music, elegance, quality service).

### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?

##While unfortunately, I was not able to implement the hyperparameters selection into the code.

## A simple solution would be to have a sample size of values that would be each one then passed into the SGD function:


*  ## Then compare each of their results 
*   ## Select the set of values that would return the best assesment values.

## Train and Evaluate Logistic Regression with TF.IDF vectors

Follow the same steps as above (i.e. evaluating count n-gram representations).

### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning for each model...

## BOW-tfidf:
"""

train_vect_idf 

train_vect_tfidf

dev_vect_idf = Compute_IDF(dev_text,dev_df)
dev_vect_idf

dev_vect_tfidf = compute_tf_IDF(dev_count, dev_vect_idf)
dev_vect_tfidf

weights_tfidf, training_loss_history_tfidf, validation_loss_history_tfidf = SGD(train_vect_tfidf, train_label,dev_vect_tfidf, dev_label, lr=0.001,
                                                              alpha=0.001, epochs=100, 
                                                              tolerance=0.0001, print_progress=True)

# training_loss_history_tfidf
# validation_loss_history_tfidf

plt.figure(figsize=(25,6))

plt.title('Cost Function Slope')
plt.plot(training_loss_history_tfidf, label='Training Loss History')
plt.plot(validation_loss_history_tfidf, label='Validation Loss History')
plt.legend(prop={'size': 16})
plt.xlabel('Number of Iterations')
plt.ylabel('Error Values')
plt.show()

X_te_count = train_vect

w_count = weights_tfidf

preds_te_count = predict_class(X_te_count, w_count)

# train_count, weights


Y_te = dev_vect

print('Accuracy:', accuracy_score(Y_te,preds_te_count))
print('Precision:', precision_score(Y_te,preds_te_count))
print('Recall:', recall_score(Y_te,preds_te_count))
print('F1-Score:', f1_score(Y_te,preds_te_count))

top_neg = w_count.argsort()[:10]
for i in top_neg:
#     print(id2word[i])
    print("Top negative: ",id_w_train[i],"\n")

top_pos = w_count.argsort()[::-1][:10]
for i in top_pos:
#     print(id2word[i])
    print("Top possitive: ",id_w_train[i],"\n")

"""## BOCN-count:"""

############ BOCN-count ############

# #TEST
# test_vocab, test_df, test_count = get_vocab(test_text, ngram_range=(1,3), token_pattern=r' ', 
#                        min_df=2, keep_topN=500, 
#                        stop_words = stop_words,char_ngrams=True)

#train
train_vocab_BOCN, train_df_BOCN, train_count_BOCN = get_vocab(train_text, ngram_range=(1,3), token_pattern=r' ', 
                       min_df=10, keep_topN=100, 
                       stop_words=stop_words,char_ngrams=True)
#Dev
dev_vocab_BOCN, dev_df_BOCN, dev_count_BOCN = get_vocab(dev_text, ngram_range=(1,3), token_pattern=r' ', 
                       min_df=10, keep_topN=100, 
                       stop_words=stop_words,char_ngrams=True)

# test_cgram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=True)for i in test_text) 
# test_cgram

# Use Extract ngram train_ngram
train_cgram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=True)for i in train_text) 
train_cgram


dev_cgram = (extract_ngrams(i,ngram_range=(1,3), token_pattern=r' ',  stop_words = stop_words, vocab=train_vocab, char_ngrams=True)for i in dev_text) 
dev_cgram

#List of Ngrams, SET_UNIGRAMS
#Train Vectorisation
print("Vectorise train text....","\n")
train_BOCN_count = vectorise(train_cgram, train_vocab_BOCN)

#Dev Vectorisation
print("Vectorise dev text....","\n")
dev_BOCN_count = vectorise(dev_cgram, dev_vocab_BOCN)

# #Test Vectorisation
# print("Vectorise test text....","\n")
# test_count = vectorise(test_ngram, test_vocab)

weights_BOCN, training_loss_history_BOCN, validation_loss_history_BOCN = SGD(train_BOCN_count, train_label,dev_BOCN_count, dev_label,
                                                                             lr=0.00001,alpha=0.001, epochs=100, 
                                                                            tolerance=0.0001, print_progress=True)

X_te_count = train_count_BOCN

w_count = weights_BOCN

preds_te_count = predict_class(X_te_count, w_count)

# train_count, weights


Y_te = dev_count_BOCN

print('Accuracy:', accuracy_score(Y_te,preds_te_count))
print('Precision:', precision_score(Y_te,preds_te_count))
print('Recall:', recall_score(Y_te,preds_te_count))
print('F1-Score:', f1_score(Y_te,preds_te_count))

# weights_BOCN, 
# training_loss_history_BOCN, 
# validation_loss_history_BOCN


plt.figure(figsize=(25,6))

plt.title('Cost Function Slope')
plt.plot(training_loss_history_BOCN, label='Training Loss History')
plt.plot(validation_loss_history_BOCN, label='Validation Loss History')
plt.legend(prop={'size': 16})
plt.xlabel('Number of Iterations')
plt.ylabel('Error Values')
plt.show()

# print("Test Dictionary >>> ", "\n")
# id_w_test, w_id_test = create_2dict(test_df)

print("Train Dictionary >>> ", "\n")
id_w_train_BOCN, w_id_train_BOCN =create_2dict(train_df_BOCN)

print("DEV Dictionary >>> ", "\n")
id_w_dev_BOCN, w_id_dev_BOCN =create_2dict(dev_df_BOCN)

top_neg = w_count.argsort()[:10]
for i in top_neg:
    print("Top negative: ",id_w_train_BOCN[i],"\n")


top_pos = w_count.argsort()[::-1][:10]
for i in top_pos:
    print("Top positive: ", id_w_train_BOCN[i],"\n")

"""## BOCN-tfidf:"""

train_BOCN_idf = Compute_IDF(train_text,train_df_BOCN)
train_BOCN_idf

train_BOCN_tfidf = compute_tf_IDF(train_count_BOCN, train_BOCN_idf)
train_BOCN_tfidf

dev_BOCN_idf = Compute_IDF(dev_text,dev_df_BOCN)
dev_BOCN_idf


dev_BOCN_tfidf = compute_tf_IDF(dev_count_BOCN, dev_BOCN_idf)
dev_BOCN_tfidf

weights_BOCN_tfidf, training_loss_history_BOCN_tfidf, validation_loss_history_BOCN_tfidf = SGD(train_BOCN_tfidf, train_label,dev_BOCN_tfidf, dev_label, 
                                                                                               lr=0.001,alpha=0.001, epochs=100, 
                                                                                               tolerance=0.0001, print_progress=True)

X_te_count = train_BOCN_tfidf

w_count = weights_BOCN_tfidf

preds_te_count = predict_class(X_te_count, w_count)

# train_count, weights


Y_te = dev_BOCN_tfidf

print('Accuracy:', accuracy_score(Y_te,preds_te_count))
print('Precision:', precision_score(Y_te,preds_te_count))
print('Recall:', recall_score(Y_te,preds_te_count))
print('F1-Score:', f1_score(Y_te,preds_te_count))

# training_loss_history
# validation_loss_history

plt.figure(figsize=(25,6))

plt.title('Cost Function Slope')
plt.plot(training_loss_history_BOCN_tfidf, label='Training Loss History')
plt.plot(validation_loss_history_BOCN_tfidf, label='Validation Loss History')
plt.legend(prop={'size': 16})
plt.xlabel('Number of Iterations')
plt.ylabel('Error Values')
plt.show()

# print("Test Dictionary >>> ", "\n")
# id_w_test_BOCN_tfidf, w_id_test_BOCN_tfidf = create_2dict(test_df_BOCN)

# print("Train Dictionary >>> ", "\n")
id_w_train_BOCN_tfidf, w_id_train_BOCN_tfidf =create_2dict(train_df_BOCN)

# print("DEV Dictionary >>> ", "\n")
id_w_dev_BOCN_tfidf, w_id_dev_BOCN_tfidf =create_2dict(dev_df_BOCN)

top_neg = w_count.argsort()[:10]
for i in top_neg:
    print("Top Negative: ",id_w_train_BOCN_tfidf[i],"\n")

top_pos = w_count.argsort()[::-1][:10]
for i in top_pos:
    print("Top Positive: ",id_w_train_BOCN_tfidf[i],"\n")

""" ## BOW+BOCN:"""



"""

## Full Results

Add here your results:

| LR | Precision  | Recall  | F1-Score  |
|:-:|:-:|:-:|:-:|
| BOW-count  |   |   |   |
| BOW-tfidf  |   |   |   |
| BOCN-count  |   |   |   |
| BOCN-tfidf  |   |   |   |
| BOW+BOCN  |   |   |   |

Please discuss why your best performing model is better than the rest."""

# Unfortunately, I was unable to get the assesment values for these models

